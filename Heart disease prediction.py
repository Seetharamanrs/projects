# -*- coding: utf-8 -*-
"""appliedAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pmnGCx9UZ832HMGNR65G-EFZ3F0Rg4H3

# 1.Dataset import

This is the same dataset that downloaded and used here we can import directly from repositary using this same as this X and y

pip install ucimlrepo


from ucimlrepo import fetch_ucirepo
  
###fetch dataset
heart_disease = fetch_ucirepo(id=45)
  
### data (as pandas dataframes)
X = heart_disease.data.features
y = heart_disease.data.targets
  
### metadata
print(heart_disease.metadata)
  
### variable information
print(heart_disease.variables)
"""

path = "/content/drive/MyDrive/applied AI/dataset/processed.cleveland.data"
columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']

"""# 2. Importing the libraries"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy  as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score,f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_curve, auc
from matplotlib.legend_handler import HandlerLine2D
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import plotly.express as px

"""# 3. Reading the data"""

data = pd.read_csv(path,names=columns,na_values='?',header=None, encoding='latin1')

"""printing few columns and row"""

data.head()

data.sample(5)

"""Data type of the data"""

data.dtypes

"""Shape of dataset"""

data.shape

"""Information of datasset"""

data. describe()

data.info()

"""# 4. Data preprocessing"""

sns.pairplot(data)

data['target'].value_counts()

sns.countplot(x='target',data=data)

"""As you can see, 1,2,3,4 are the same as having heart disease, thus replace the 2,3,4 to the integer 1 to have heart disease."""

data['target']= data['target'].replace([2,3,4],1)

data['target'].value_counts()

sns.countplot(x='target',data=data)

data.sample(10)

"""check the precentage of how many people has heart disease and not having a heart disease"""

fig = px.pie(data,values='sex', names='target',title='Gender distribution in heart health')
fig.update_layout(height=400, width=400)
fig.show()

fig = px.pie(data, values='thal', names='cp',title='distribution of thalasemia with respect to chest pain')
fig.update_layout(height=400, width=600)
fig.show()

plt.figure(figsize=(5,5))
sns.scatterplot(data=data,y='age',x='target')

"""correlation matix of the feature"""

plt.figure(figsize=(20,10))
sns.heatmap(data.corr(),vmin=-1,cmap='coolwarm',annot=True)

def correlation(dataset,threshold):
  col_corr=set()
  corr_matrix = dataset.corr()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
      if abs(corr_matrix.iloc[i,j])> threshold:
        colname=corr_matrix.columns[i]
        col_corr.add(colname)
  return col_corr

z= data.drop(columns='target')
corr_features = correlation(z,0.80)
len(set(corr_features))

"""# 5. Understand the  dataset columns better

**•	Age:** The person's age in years

**•	sex:** The person's sex (1 = male; 0 = female)

**•	cp:**  The chest pain experienced(Value 1: typical angina,Value 2: atypical angina,Value 3: non-anginal pain,Value 4: asymptomatic)

**• trestbps:** The person's resting blood pressure (in mm Hg on admission to the hospital)

**• chol:** The person's serum cholestoral in mg/dl

**• fbs:** The person's fasting blood sugar > 120 mg/dl (1 = true; 0 = false)

**• restecg:** Resting electrocardiographic results (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)

**• thalach:** The person's maximum heart rate achieved

**• exang:** Exercise induced angina

**• oldpeak:** ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot.)

**• slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)

**• ca:** The number of major vessels (0-3)

**• thal:** A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)

**• target:** Heart disease (0 = not present, 1= present )

To better understanding, Given the full name to the columns
"""

data.columns=['age','sex','chestpain','resting_blood_pressure','serum_cholestoral','fasting_blood_sugar','resting_ecg','max_heart_rate_achieved','exercise_induced_angina','ST_depression','ST_slope','number_of_major_vessels','thalassemia','target']

"""finding the missing values  """

data.isnull().sum()

"""### Trying to fill the columns

*   we can see in above result that "thalassemia","number_of_major_vessels" have a missing values as 2 and 4 respectively

*   So trying to fill the value by calculating mean of the column and that mean value is used to fill in the missing values in the column
"""

data['number_of_major_vessels' ]=data['number_of_major_vessels' ].fillna(data['number_of_major_vessels'].mean())
data['thalassemia' ]=data['thalassemia' ].fillna(data['thalassemia'].mean())

"""# 6. Algorithm implementation"""

X=data.drop(columns='target')
y = data['target']

"""spliting data into training and testing data"""

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=46)

print(X.shape,X_train.shape,X_test.shape)

"""## Hyper parameter tuning"""

n_estimators=[5,10,20,50,100,200,300]
train_results=[]
test_results=[]
for estimator in n_estimators:
  rf= RandomForestClassifier(n_estimators=estimator ,n_jobs=-1,random_state=25)

  rf.fit(X_train,y_train)

  train_pred=rf.predict(X_train)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_train,train_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  train_results.append(roc_auc)

  y_pred =rf.predict(X_test)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_test,y_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  test_results.append(roc_auc)

for feature, train_roc_auc, test_roc_auc in zip(n_estimators, train_results, test_results):
    print(f"n_estimators: {feature}, Train AUC: {train_roc_auc:.4f}, Test AUC: {test_roc_auc:.4f}")

# Find the best n_estimators
best_n_estimators = n_estimators[test_results.index(max(test_results))]
print(f"\nBest n_estimators: {best_n_estimators}")

line1,=plt.plot(n_estimators,train_results,"b",label='train AUC')
line2,=plt.plot(n_estimators,test_results,"r",label='Test AUC')
plt.legend(handler_map={line1:HandlerLine2D(numpoints=2)})
plt.ylabel("AUC score")
plt.xlabel("n_estimators")
plt.show()

max_depth=[1,2,3,4,5,6,7,10,20,30]
train_results=[]
test_results=[]
for estimator in max_depth:
  rf= RandomForestClassifier(n_estimators =10,max_depth=estimator ,n_jobs=-1,random_state=25)

  rf.fit(X_train,y_train)

  train_pred=rf.predict(X_train)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_train,train_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  train_results.append(roc_auc)

  y_pred =rf.predict(X_test)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_test,y_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  test_results.append(roc_auc)

for feature, train_roc_auc, test_roc_auc in zip(max_depth, train_results, test_results):
    print(f"max_depth: {feature}, Train AUC: {train_roc_auc:.4f}, Test AUC: {test_roc_auc:.4f}")

# Find the best max_depth
best_max_depth = max_depth[test_results.index(max(test_results))]
print(f"\nBest max_depth: {best_max_depth}")

line1,=plt.plot(max_depth,train_results,"b",label='train AUC')
line2,=plt.plot(max_depth,test_results,"r",label='Test AUC')
plt.legend(handler_map={line1:HandlerLine2D(numpoints=2)})
plt.ylabel("AUC score")
plt.xlabel("max_depth")
plt.show()

max_features=[1,2,3,4,5,6,7]
train_results=[]
test_results=[]
for estimator in max_features:
  rf= RandomForestClassifier(n_estimators =10,max_depth=20,max_features=estimator ,n_jobs=-1,random_state=25)

  rf.fit(X_train,y_train)

  train_pred=rf.predict(X_train)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_train,train_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  train_results.append(roc_auc)

  y_pred =rf.predict(X_test)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_test,y_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  test_results.append(roc_auc)

for feature, train_roc_auc, test_roc_auc in zip(max_features, train_results, test_results):
    print(f"max_features: {feature}, Train AUC: {train_roc_auc:.4f}, Test AUC: {test_roc_auc:.4f}")

# Find the best max_features
best_max_features = max_features[test_results.index(max(test_results))]
print(f"\nBest max_features: {best_max_features}")

line1,=plt.plot(max_features,train_results,"b",label='train AUC')
line2,=plt.plot(max_features,test_results,"r",label='Test AUC')
plt.legend(handler_map={line1:HandlerLine2D(numpoints=2)})
plt.ylabel("AUC score")
plt.xlabel("max_features")
plt.show()

min_samples_leaf=[1,2,3,4,5,6,7]
train_results=[]
test_results=[]
for estimator in min_samples_leaf:
  rf= RandomForestClassifier(n_estimators =10,max_depth=20,max_features=3,min_samples_leaf=estimator,n_jobs=-1,random_state=25)

  rf.fit(X_train,y_train)

  train_pred=rf.predict(X_train)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_train,train_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  train_results.append(roc_auc)

  y_pred =rf.predict(X_test)
  false_positive_rate,true_positive_rate,thersholds=roc_curve(y_test,y_pred)
  roc_auc=auc(false_positive_rate,true_positive_rate)

  test_results.append(roc_auc)

for feature, train_roc_auc, test_roc_auc in zip(min_samples_leaf, train_results, test_results):
    print(f"min_samples_leaf: {feature}, Train AUC: {train_roc_auc:.4f}, Test AUC: {test_roc_auc:.4f}")

# Find the best min_samples_leaf
best_min_samples_leaf = min_samples_leaf[test_results.index(max(test_results))]
print(f"\nBest min_samples_leaf: {best_min_samples_leaf}")

line1,=plt.plot(min_samples_leaf,train_results,"b",label='train AUC')
line2,=plt.plot(min_samples_leaf,test_results,"r",label='Test AUC')
plt.legend(handler_map={line1:HandlerLine2D(numpoints=2)})
plt.ylabel("AUC score")
plt.xlabel("min_samples_leaf")
plt.show()

"""#Random forest model"""

# creating model
rf = RandomForestClassifier(n_estimators=10,max_depth=20,max_features=3,min_samples_leaf=1,random_state=25) # 100 decision tree

# fitting training data
rf=rf.fit(X_train,y_train)

# Prediction on training data
rf_train_predict = rf.predict(X_train)


# Prediction on testing data
rf_test_predict = rf.predict(X_test)

rf_train_accuracy=accuracy_score(rf_train_predict,y_train)
print("Training accuracy score of random forest",round(rf_train_accuracy*100,2),'%')

rf_test_accuracy = accuracy_score(rf_test_predict,y_test)
print("Testing accuracy score  of random forest",round(rf_test_accuracy*100,2),'%')

print(classification_report(y_test,rf_test_predict))

matrix=confusion_matrix(y_test,rf_test_predict)
sns.heatmap(matrix/np.sum(matrix),fmt='.2%',annot=True)

"""# K Nearest Neighbors"""

# creating the model
knn = KNeighborsClassifier(n_neighbors=7)

#Fitting the traning data
knn.fit(X_train,y_train)

#Predicting the tes data
y_pred_knn=knn.predict(X_test)

#predicting the training data
y_pred=knn.predict(X_train)

score_knn_train = accuracy_score(y_pred,y_train)
print("Training accuracy of KNN is: ",round(score_knn_train*100,2),"%")

score_knn = accuracy_score(y_pred_knn,y_test)
print("Testing accuracy of KNN is: ",round(score_knn*100,2),"%")

print(classification_report(y_test,y_pred_knn))

matrix=confusion_matrix(y_test,y_pred_knn)
sns.heatmap(matrix/np.sum(matrix),fmt='.2%',annot=True)

alogrithm_names= ['random forest ', 'K nearest neighbour']

plt.bar(alogrithm_names, [round(rf_test_accuracy*100,2), round(score_knn*100,2)], color=['green', 'blue'])

# Adding labels and title
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.title('Algorithm Accuracy Comparison')

# Display the plot
plt.show()

"""#Buliding a predictive system"""

input=(58.0,0.0,2.0,136.0,319.0,1.0,2.0,152.0,0.0,0.0,1.0,2.0,3.0)   #answer should be 1 for  this case

#changing input dats toa a numpy array
input_data=np.asarray(input)

#reshape the numpy array as we are predicting for one data
input_data_reshape=input_data.reshape(1,-1)

prediction = knn.predict(input_data_reshape)

if (prediction[0]==0):
  print("the person don't have heart disease")
else:
  print('the person has heart disease')